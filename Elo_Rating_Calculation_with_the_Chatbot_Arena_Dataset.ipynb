{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizzy777123/AMYs-Portfolio/blob/main/Elo_Rating_Calculation_with_the_Chatbot_Arena_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyl5Vil7HRzd"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook, we will perform visualizations and Elo rating calculations using the released chatbot arena conversation dataset.(https://huggingface.co/datasets/lmsys/chatbot_arena_conversations).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C5H_wlbqGwCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30101745-e6f7-4807-f4fd-57b22238fa7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n"
          ]
        }
      ],
      "source": [
        "# import packages\n",
        "!pip install datasets\n",
        "\n",
        "from collections import defaultdict\n",
        "import json, math, gdown\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "pd.options.display.float_format = '{:.2f}'.format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ0G_G-sHwm3"
      },
      "source": [
        "# Obtaining and Cleaning the Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "cWr1RMsqxWGg",
        "outputId": "e710609a-f2df-42f1-b8aa-078a0faad47a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-32c8d45d51f9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lmsys/chatbot_arena_conversations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2127\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2128\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2129\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2130\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1815\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1505\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m                     raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1508\u001b[0m                         \u001b[0;34mf\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m                         \u001b[0;34mf\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find a dataset script at /content/lmsys/chatbot_arena_conversations/chatbot_arena_conversations.py or any data file in the same directory. Couldn't find 'lmsys/chatbot_arena_conversations' on the Hugging Face Hub either: FileNotFoundError: Dataset 'lmsys/chatbot_arena_conversations' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`."
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"lmsys/chatbot_arena_conversations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0rg26TQxFQv"
      },
      "outputs": [],
      "source": [
        "rows = dataset[\"train\"].to_json(\"tmp.json\")\n",
        "battles = pd.read_json(\"tmp.json\", lines=True).sort_values(ascending=True, by=[\"tstamp\"])\n",
        "battles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IdpT27Q8IE_"
      },
      "source": [
        "# Exploratory Analysis\n",
        "\n",
        "Before computing the Elo ratings, we first conduct some basic exploratory analysis to highlight a few key properties and caveates with this data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioFGqU_O8a1f"
      },
      "source": [
        "## Signfiicant Number of Ties\n",
        "\n",
        "We allowed the user to declare a tie between the pairs of models.  To collect additional data, later in the tournament we also allowed the user to declare a tie in which both models were bad.  There were a significant number of tied outcomes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mcfYn7dIPVl"
      },
      "outputs": [],
      "source": [
        "fig = px.bar(battles[\"winner\"].value_counts(),\n",
        "             title=\"Counts of Battle Outcomes\", text_auto=True, height=400)\n",
        "fig.update_layout(xaxis_title=\"Battle Outcome\", yaxis_title=\"Count\",\n",
        "                  showlegend=False)\n",
        "fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csZ2iiW4SDN3"
      },
      "outputs": [],
      "source": [
        "battles_no_ties = battles[~battles[\"winner\"].str.contains(\"tie\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWtkYOdR1LQl"
      },
      "source": [
        "## Non-uniform Model Frequency\n",
        "\n",
        "The model frequency is not uniform because of the follwoing reasons:\n",
        "- Several different matching and sampling algorithms were used. We employed uniform sampling as well as weighted sampling methods, which assign greater weights to better models.\n",
        "- Some new models were added later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4vYYIjlzbLk"
      },
      "outputs": [],
      "source": [
        "fig = px.bar(pd.concat([battles[\"model_a\"], battles[\"model_b\"]]).value_counts(),\n",
        "             title=\"Battle Count for Each Model\", text_auto=True)\n",
        "fig.update_layout(xaxis_title=\"model\", yaxis_title=\"Battle Count\", height=400,\n",
        "                  showlegend=False)\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wCkDV405usE"
      },
      "source": [
        "We examing the number of pairings for each combination of models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyC00Fyv10Bs"
      },
      "outputs": [],
      "source": [
        "def visualize_battle_count(battles, title):\n",
        "    ptbl = pd.pivot_table(battles, index=\"model_a\", columns=\"model_b\", aggfunc=\"size\",\n",
        "                          fill_value=0)\n",
        "    battle_counts = ptbl + ptbl.T\n",
        "    ordering = battle_counts.sum().sort_values(ascending=False).index\n",
        "    fig = px.imshow(battle_counts.loc[ordering, ordering],\n",
        "                    title=title, text_auto=True, width=600)\n",
        "    fig.update_layout(xaxis_title=\"Model B\",\n",
        "                      yaxis_title=\"Model A\",\n",
        "                      xaxis_side=\"top\", height=600, width=600,\n",
        "                      title_y=0.07, title_x=0.5)\n",
        "    fig.update_traces(hovertemplate=\n",
        "                      \"Model A: %{y}<br>Model B: %{x}<br>Count: %{z}<extra></extra>\")\n",
        "    return fig\n",
        "\n",
        "fig = visualize_battle_count(battles, title=\"Battle Count of Each Combination of Models\")\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwAqPHcK0Tu9"
      },
      "source": [
        "### Battles Excluding Ties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0vh1TcQ0YBd"
      },
      "outputs": [],
      "source": [
        "visualize_battle_count(battles_no_ties, \"Battle Count for Each Combination of Models (without Ties)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqSGI-dA-3By"
      },
      "source": [
        "### Counting Ties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cemVcoOMQEol"
      },
      "outputs": [],
      "source": [
        "visualize_battle_count(battles[battles['winner'].str.contains(\"tie\")], \"Tie Count for Each Combination of Models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgWNuDWy6jE6"
      },
      "source": [
        "## Inferred Language\n",
        "\n",
        "We also inferred the language for each conversation using `polyglot` package. This is just an estimate but will help guide future analysis.  The vast majority of conversations were in English."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5-yrGGPIZ9y"
      },
      "outputs": [],
      "source": [
        "topk = 15\n",
        "fig = px.bar(battles[\"language\"].value_counts().head(topk),\n",
        "             title=f\"Battle Counts for the Top {topk} Languages\",\n",
        "             text_auto=True, height=400)\n",
        "fig.update_layout(xaxis_title=\"Language\", yaxis_title=\"Count\", showlegend=False)\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-eu9nhUqUYB"
      },
      "source": [
        "## Number of Conversation Turns\n",
        "\n",
        "We also noticed that most counversations only have one turn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaNaDmasqcnH"
      },
      "outputs": [],
      "source": [
        "fig = px.histogram(battles[\"turn\"],\n",
        "             title=f\"Number of Conversation Turns\",\n",
        "             text_auto=True, height=400)\n",
        "fig.update_layout(xaxis_title=\"Turns\", yaxis_title=\"Count\", showlegend=False)\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViV11W8l9NfL"
      },
      "source": [
        "## Pairwise Win Fractions\n",
        "\n",
        "Finally, we can also compute the pairwise win fractions. However, because each model can play as Model A and as Model B and win in both situations we need to compute the wins in both configurations divided by the number of pairings of each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHZtIRD5JRHG"
      },
      "outputs": [],
      "source": [
        "def compute_pairwise_win_fraction(battles):\n",
        "    # Times each model wins as Model A\n",
        "    a_win_ptbl = pd.pivot_table(\n",
        "        battles[battles['winner'] == \"model_a\"],\n",
        "        index=\"model_a\", columns=\"model_b\", aggfunc=\"size\", fill_value=0)\n",
        "\n",
        "    # Table counting times each model wins as Model B\n",
        "    b_win_ptbl = pd.pivot_table(\n",
        "        battles[battles['winner'] == \"model_b\"],\n",
        "        index=\"model_a\", columns=\"model_b\", aggfunc=\"size\", fill_value=0)\n",
        "\n",
        "    # Table counting number of A-B pairs\n",
        "    num_battles_ptbl = pd.pivot_table(battles,\n",
        "        index=\"model_a\", columns=\"model_b\", aggfunc=\"size\", fill_value=0)\n",
        "\n",
        "    # Computing the proportion of wins for each model as A and as B\n",
        "    # against all other models\n",
        "    row_beats_col_freq = (\n",
        "        (a_win_ptbl + b_win_ptbl.T) /\n",
        "        (num_battles_ptbl + num_battles_ptbl.T)\n",
        "    )\n",
        "\n",
        "    # Arrange ordering according to proprition of wins\n",
        "    prop_wins = row_beats_col_freq.mean(axis=1).sort_values(ascending=False)\n",
        "    model_names = list(prop_wins.keys())\n",
        "    row_beats_col = row_beats_col_freq.loc[model_names, model_names]\n",
        "    return row_beats_col\n",
        "\n",
        "def visualize_pairwise_win_fraction(battles, title):\n",
        "    row_beats_col = compute_pairwise_win_fraction(battles)\n",
        "    fig = px.imshow(row_beats_col, color_continuous_scale='RdBu',\n",
        "                    text_auto=\".2f\", title=title)\n",
        "    fig.update_layout(xaxis_title=\" Model B: Loser\",\n",
        "                  yaxis_title=\"Model A: Winner\",\n",
        "                  xaxis_side=\"top\", height=600, width=600,\n",
        "                  title_y=0.07, title_x=0.5)\n",
        "    fig.update_traces(hovertemplate=\n",
        "                  \"Model A: %{y}<br>Model B: %{x}<br>Fraction of A Wins: %{z}<extra></extra>\")\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhGcfNz-Luni"
      },
      "outputs": [],
      "source": [
        "fig = visualize_pairwise_win_fraction(battles_no_ties,\n",
        "      title = \"Fraction of Model A Wins for All Non-tied A vs. B Battles\")\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq3NlxirIyb7"
      },
      "source": [
        "## Preliminary Ranking\n",
        "\n",
        "Using just the average win rate against all other models we can already compute an estimated leaderboard.\n",
        "However, this method may not be as scalable as the Elo rating system that we will use later because this method requires data from all model combinations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0RzCkEVIR9c"
      },
      "outputs": [],
      "source": [
        "row_beats_col_freq = compute_pairwise_win_fraction(battles_no_ties)\n",
        "fig = px.bar(row_beats_col_freq.mean(axis=1).sort_values(ascending=False),\n",
        "             title=\"Average Win Rate Against All Other Models (Assuming Uniform Sampling and No Ties)\",\n",
        "             text_auto=\".2f\")\n",
        "fig.update_layout(yaxis_title=\"Average Win Rate\", xaxis_title=\"Model\",\n",
        "                  showlegend=False)\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_PYA7oVyaHO"
      },
      "source": [
        "#Elo Ratings\n",
        "\n",
        "The [Elo rating system ](https://en.wikipedia.org/wiki/Elo_rating_system)is a method for calculating the relative skill levels of players, which has been widely adopted in chess and other competitive games. The difference in the ratings between two players serves as a predictor of the outcome of a match. The Elo rating system works well for our case because we have multiple models and we run pairwise battles between them.\n",
        "In this section, we present different methods for calculating Elo ratings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLGc6DwxyvQc"
      },
      "source": [
        "### Compute Ratings\n",
        "We first use the online linear update algorithm to compute Elo ratings.\n",
        "We choose a small K-factor of 4 to make the Elo ratings more stable and less biased towards recent games.\n",
        "\n",
        "However, even with a small K-factor, we still found this online update algoirhtm not stable enough, so we did not use this version directly for our leaderboard, but will use a bootstrap version of this later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hytEb0aXfcwm"
      },
      "outputs": [],
      "source": [
        "def compute_elo(battles, K=4, SCALE=400, BASE=10, INIT_RATING=1000):\n",
        "    rating = defaultdict(lambda: INIT_RATING)\n",
        "\n",
        "    for rd, model_a, model_b, winner in battles[['model_a', 'model_b', 'winner']].itertuples():\n",
        "        ra = rating[model_a]\n",
        "        rb = rating[model_b]\n",
        "        ea = 1 / (1 + BASE ** ((rb - ra) / SCALE))\n",
        "        eb = 1 / (1 + BASE ** ((ra - rb) / SCALE))\n",
        "        if winner == \"model_a\":\n",
        "            sa = 1\n",
        "        elif winner == \"model_b\":\n",
        "            sa = 0\n",
        "        elif winner == \"tie\" or winner == \"tie (bothbad)\":\n",
        "            sa = 0.5\n",
        "        else:\n",
        "            raise Exception(f\"unexpected vote {winner}\")\n",
        "        rating[model_a] += K * (sa - ea)\n",
        "        rating[model_b] += K * (1 - sa - eb)\n",
        "\n",
        "    return rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3v8wc_oCmtmW"
      },
      "outputs": [],
      "source": [
        "def preety_print_elo_ratings(ratings):\n",
        "    df = pd.DataFrame([\n",
        "        [n, elo_ratings[n]] for n in elo_ratings.keys()\n",
        "    ], columns=[\"Model\", \"Elo rating\"]).sort_values(\"Elo rating\", ascending=False).reset_index(drop=True)\n",
        "    df[\"Elo rating\"] = (df[\"Elo rating\"] + 0.5).astype(int)\n",
        "    df.index = df.index + 1\n",
        "    return df\n",
        "\n",
        "elo_ratings = compute_elo(battles)\n",
        "preety_print_elo_ratings(elo_ratings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92pAnooyzBCG"
      },
      "source": [
        "### Compute Bootstrap Confidence Interavals for Elo Scores\n",
        "\n",
        "The previous linear update method may be sensitive to battle orders. Here, we use bootstrap to get a more stable versoion and estimate the confidence intervals as well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9Plp9KhAu2n"
      },
      "outputs": [],
      "source": [
        "def get_bootstrap_result(battles, func_compute_elo, num_round):\n",
        "    rows = []\n",
        "    for i in tqdm(range(num_round), desc=\"bootstrap\"):\n",
        "        rows.append(func_compute_elo(battles.sample(frac=1.0, replace=True)))\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df[df.median().sort_values(ascending=False).index]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZbrbbbRiV95"
      },
      "outputs": [],
      "source": [
        "BOOTSTRAP_ROUNDS = 1000\n",
        "\n",
        "np.random.seed(42)\n",
        "bootstrap_elo_lu = get_bootstrap_result(battles, compute_elo, BOOTSTRAP_ROUNDS)\n",
        "bootstrap_lu_median = bootstrap_elo_lu.median().reset_index().set_axis([\"model\", \"Elo rating\"], axis=1)\n",
        "bootstrap_lu_median[\"Elo rating\"] = (bootstrap_lu_median[\"Elo rating\"] + 0.5).astype(int)\n",
        "bootstrap_lu_median"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njg7wkbuZbHx"
      },
      "outputs": [],
      "source": [
        "def visualize_bootstrap_scores(df, title):\n",
        "    bars = pd.DataFrame(dict(\n",
        "        lower = df.quantile(.025),\n",
        "        rating = df.quantile(.5),\n",
        "        upper = df.quantile(.975))).reset_index(names=\"model\").sort_values(\"rating\", ascending=False)\n",
        "    bars['error_y'] = bars['upper'] - bars[\"rating\"]\n",
        "    bars['error_y_minus'] = bars['rating'] - bars[\"lower\"]\n",
        "    bars['rating_rounded'] = np.round(bars['rating'], 2)\n",
        "    fig = px.scatter(bars, x=\"model\", y=\"rating\", error_y=\"error_y\",\n",
        "                     error_y_minus=\"error_y_minus\", text=\"rating_rounded\",\n",
        "                     title=title)\n",
        "    fig.update_layout(xaxis_title=\"Model\", yaxis_title=\"Rating\")\n",
        "    return fig\n",
        "\n",
        "fig = visualize_bootstrap_scores(bootstrap_elo_lu, \"Bootstrap of Elo Estimates\")\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsk4LxTWTxRx"
      },
      "source": [
        "### Predict Win Rates\n",
        "Utilizing Elo ratings allows us to predict win probabilities. By comparing the predicted win rates with the actual win rates, we can gain insight into the accuracy and quality of the Elo rating system.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0ZH43bEeVqY"
      },
      "outputs": [],
      "source": [
        "def predict_win_rate(elo_ratings, SCALE=400, BASE=10, INIT_RATING=1000):\n",
        "    names = sorted(list(elo_ratings.keys()))\n",
        "    wins = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "    for a in names:\n",
        "        for b in names:\n",
        "            ea = 1 / (1 + BASE ** ((elo_ratings[b] - elo_ratings[a]) / SCALE))\n",
        "            wins[a][b] = ea\n",
        "            wins[b][a] = 1 - ea\n",
        "\n",
        "    data = {\n",
        "        a: [wins[a][b] if a != b else np.NAN for b in names]\n",
        "        for a in names\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data, index=names)\n",
        "    df.index.name = \"model_a\"\n",
        "    df.columns.name = \"model_b\"\n",
        "    return df.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0FkQPRxQyi6"
      },
      "outputs": [],
      "source": [
        "win_rate = predict_win_rate(dict(bootstrap_elo_lu.quantile(0.5)))\n",
        "ordered_models = win_rate.mean(axis=1).sort_values(ascending=False).index\n",
        "fig = px.imshow(win_rate.loc[ordered_models, ordered_models],\n",
        "                color_continuous_scale='RdBu', text_auto=\".2f\",\n",
        "                title=\"Predicted Win Rate Using Elo Ratings for Model A in an A vs. B Battle\")\n",
        "fig.update_layout(xaxis_title=\"Model B\",\n",
        "                  yaxis_title=\"Model A\",\n",
        "                  xaxis_side=\"top\", height=600, width=600,\n",
        "                  title_y=0.07, title_x=0.5)\n",
        "fig.update_traces(hovertemplate=\n",
        "                  \"Model A: %{y}<br>Model B: %{x}<br>Win Rate: %{z}<extra></extra>\")\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lBa9pdtzMA3"
      },
      "source": [
        "### Compute Bootstrap Confidence Intervals Assuming Uniform Sampling\n",
        "\n",
        "We also study how the ratings will change if we only sample an equal number of battles for each model pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTwWk15zZSTh"
      },
      "outputs": [],
      "source": [
        "def sample_battle_even(battles, n_per_battle):\n",
        "    groups = battles.groupby([\"model_a\", \"model_b\"], as_index=False)\n",
        "    resampled = (groups\n",
        "                 .apply(lambda grp: grp.sample(n_per_battle, replace=True))\n",
        "                 .reset_index(drop=True))\n",
        "    return resampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM_zlH-yZRhz"
      },
      "outputs": [],
      "source": [
        "num_samples = 50\n",
        "battles_even = sample_battle_even(battles, num_samples)\n",
        "pd.pivot_table(battles_even, index=\"model_a\", columns=\"model_b\", aggfunc=\"size\", fill_value=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hkd4dxRR-DI"
      },
      "outputs": [],
      "source": [
        "# Sampling Battles Evenly\n",
        "def get_bootstrap_even_sample(battles, n_per_battle, func_compute_elo, num_round=BOOTSTRAP_ROUNDS):\n",
        "    rows = []\n",
        "    for n in tqdm(range(num_round), desc=\"sampling battles evenly\"):\n",
        "        resampled = sample_battle_even(battles, n_per_battle)\n",
        "        rows.append(func_compute_elo(resampled))\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df[df.median().sort_values(ascending=False).index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHfd6btNXYKt"
      },
      "outputs": [],
      "source": [
        "# num_samples = int(np.min(pd.pivot_table(battles, index=\"model_a\", columns=\"model_b\", aggfunc=\"size\", fill_value=1e10).values))\n",
        "print(\"number of samples per battle pair:\", num_samples)\n",
        "bootstrap_even_lu = get_bootstrap_even_sample(battles, num_samples, compute_elo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJWnGH9c8Q3g"
      },
      "outputs": [],
      "source": [
        "fig = visualize_bootstrap_scores(bootstrap_even_lu, f\"Bootstrap of Elo Estimates - Even sample\")\n",
        "fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fseJo62TJCT7"
      },
      "outputs": [],
      "source": [
        "px.violin(bootstrap_even_lu.melt(), x=\"variable\", y=\"value\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdiJbB6pZB1B"
      },
      "source": [
        "### Maximum Likelihood Estimation\n",
        "Another way to fit Elo ratings is using maximum likelihood estimation. Here, we provide an impelmentation with logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbKV7-s9ZA_5"
      },
      "outputs": [],
      "source": [
        "def compute_elo_mle(df, SCALE=400, BASE=10, INIT_RATING=1000):\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    models = pd.concat([df[\"model_a\"], df[\"model_b\"]]).unique()\n",
        "    models = pd.Series(np.arange(len(models)), index=models)\n",
        "    p = len(models.index)\n",
        "    n = df.shape[0]\n",
        "\n",
        "    X = np.zeros([n, p])\n",
        "    X[np.arange(n), models[df[\"model_a\"]]] = +math.log(BASE)\n",
        "    X[np.arange(n), models[df[\"model_b\"]]] = -math.log(BASE)\n",
        "\n",
        "    Y = np.zeros(n)\n",
        "    Y[df[\"winner\"] == \"model_a\"] = 1.0\n",
        "\n",
        "    lr = LogisticRegression(fit_intercept=False)\n",
        "    lr.fit(X,Y)\n",
        "\n",
        "    elo_scores = SCALE * lr.coef_[0] + INIT_RATING\n",
        "\n",
        "    return pd.Series(elo_scores, index = models.index).sort_values(ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRiG5qkVZN4Z"
      },
      "outputs": [],
      "source": [
        "compute_elo_mle(battles_no_ties)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSkA2LzoZQpO"
      },
      "outputs": [],
      "source": [
        "elo_mle_bootstrap = get_bootstrap_result(battles_no_ties, compute_elo_mle, 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efTLYOcnZTCo"
      },
      "outputs": [],
      "source": [
        "visualize_bootstrap_scores(elo_mle_bootstrap, \"Bootstrap of MLE Elo Estimates\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAI96WZJ9fxQ"
      },
      "source": [
        "# Language-specific Leaderboards\n",
        "We present two language-specific leaderboards, by isolating the chat data into two subsets based on the language: (1) English-only and (2) Non-English."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a9ZUqPC9l0-"
      },
      "source": [
        "## English-only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBnS0fCL9wE2"
      },
      "outputs": [],
      "source": [
        "english_only_battles = battles[battles[\"language\"] == \"English\"]\n",
        "elo_ratings = compute_elo(english_only_battles)\n",
        "preety_print_elo_ratings(elo_ratings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn7OZDdh9noH"
      },
      "source": [
        "## Non-English"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxIKKbif9wuA"
      },
      "outputs": [],
      "source": [
        "non_english_battles = battles[battles[\"language\"] != \"English\"]\n",
        "elo_ratings = compute_elo(non_english_battles)\n",
        "preety_print_elo_ratings(elo_ratings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGwWvngLONeS"
      },
      "source": [
        "# Links\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi-X5I7UPkxU"
      },
      "source": [
        "\n",
        "Some good resources to learn more about Elo rating systems:\n",
        "- Wikipedia https://en.wikipedia.org/wiki/Elo_rating_system\n",
        "- An introduction video https://www.youtube.com/watch?v=AsYfbmp0To0\n",
        "- A FiveThirtyEight article https://fivethirtyeight.com/methodology/how-our-nfl-predictions-work/\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "bGwWvngLONeS"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}